{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXixzxqdT73K9yRJdrxecZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmolmatharu/CareQuery-AI/blob/main/CareQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBDRLInDp5ij",
        "outputId": "da698b81-fee4-409f-f94e-2ff1259f6a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-textanalytics)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2025.1.31)\n",
            "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.32.0 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "!pip install azure-ai-textanalytics\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MEDICAL_ABBREVIATIONS = {\n",
        "    \"pt\": \"patient\",\n",
        "    \"c/o\": \"complains of\",\n",
        "    \"sob\": \"shortness of breath\",\n",
        "    \"cp\": \"chest pain\",\n",
        "    \"hx\": \"history\",\n",
        "    \"dx\": \"diagnosis\",\n",
        "    \"tx\": \"treatment\",\n",
        "    \"fx\": \"fracture\",\n",
        "    \"abd\": \"abdominal\",\n",
        "    \"hr\": \"heart rate\",\n",
        "    \"bp\": \"blood pressure\",\n",
        "    \"temp\": \"temperature\",\n",
        "    \"lab\": \"laboratory\",\n",
        "    \"meds\": \"medications\",\n",
        "    \"pm\": \"after meals\",\n",
        "    \"prn\": \"as needed\",\n",
        "    \"stat\": \"immediately\",\n",
        "    \"bid\": \"twice daily\",\n",
        "    \"tid\": \"three times daily\",\n",
        "    \"qid\": \"four times daily\",\n",
        "    \"po\": \"by mouth\",\n",
        "    \"iv\": \"intravenous\",\n",
        "    \"im\": \"intramuscular\",\n",
        "    \"sc\": \"subcutaneous\",\n",
        "    \"hs\": \"at bedtime\",\n",
        "    \"yo\": \"year old\",\n",
        "    \"y/o\": \"year old\",\n",
        "    \"f/u\": \"follow up\",\n",
        "}"
      ],
      "metadata": {
        "id": "MgAhxuV3qFki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure credentials setup\n",
        "def get_azure_text_analytics_client():\n",
        "    # i used my own when running\n",
        "    #INSERT KEYS\n",
        "    key = os.environ.get(\"AZURE_TEXT_ANALYTICS_KEY\", \"your_key_here\")\n",
        "    endpoint = os.environ.get(\"AZURE_TEXT_ANALYTICS_ENDPOINT\", \"your_endpoint_here\")\n",
        "\n",
        "    # Create a client\n",
        "    credential = AzureKeyCredential(key)\n",
        "    client = TextAnalyticsClient(endpoint=endpoint, credential=credential)\n",
        "    return client"
      ],
      "metadata": {
        "id": "zv6fNzQvqLP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask application setup\n",
        "#TO RUN insert YOUR FLASK\n",
        "app = Flask(__name__)"
      ],
      "metadata": {
        "id": "fMYbNPI_qOIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.before_first_request\n",
        "def load_models():\n",
        "    global grammar_model, medical_standardization_model\n",
        "\n",
        "    #Using this NLP for transformer based architec\n",
        "    # Load BERT model from TensorFlow Hub for grammar correction\n",
        "    print(\"Loading grammar correction model...\")\n",
        "    bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "    bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "\n",
        "    # Create grammar correction model\n",
        "    text_input = Input(shape=(), dtype=tf.string, name='text_input')\n",
        "    preprocessed_text = bert_preprocess(text_input)\n",
        "    outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "    # Use the pooled output for sequence classification\n",
        "    pooled_output = outputs[\"pooled_output\"]\n",
        "    dropout = Dropout(0.1)(pooled_output)\n",
        "    grammar_output = Dense(1, activation='sigmoid', name='grammar_output')(dropout)\n",
        "\n",
        "    grammar_model = Model(inputs=[text_input], outputs=[grammar_output])\n",
        "    grammar_model.compile(optimizer=Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # USE for later on in deployment:save for now load pre-trained weights\n",
        "    # grammar_model.load_weights('grammar_model_weights.h5')\n",
        "\n",
        "    #customization\n",
        "    # Custom model for medical standardization using BERT embeddings\n",
        "    print(\" medical standardization model is loading...\")\n",
        "    medical_input = Input(shape=(), dtype=tf.string, name='medical_input')\n",
        "    preprocessed_medical = bert_preprocess(medical_input)\n",
        "    medical_outputs = bert_encoder(preprocessed_medical)\n",
        "\n",
        "    # Use the sequence output for token-level classification\n",
        "    sequence_output = medical_outputs[\"sequence_output\"]\n",
        "    dropout_seq = Dropout(0.1)(sequence_output)\n",
        "    medical_output = Dense(768, activation='relu')(dropout_seq)\n",
        "    medical_output = Dense(256, activation='relu')(medical_output)\n",
        "    medical_output = Dense(128, name='medical_term_encoding')(medical_output)\n",
        "\n",
        "    medical_standardization_model = Model(inputs=[medical_input], outputs=[medical_output])\n",
        "    # medical_standardization_model.load_weights('medical_model_weights.h5')\n",
        "\n",
        "    print(\"yayy models loaded successfully!\")"
      ],
      "metadata": {
        "id": "EiEb4SgPqQQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "mfN2ap-qqWnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_abbreviations(text):\n",
        "    \"\"\"Expand common medical abbreviations\"\"\"\n",
        "\n",
        "\n",
        "    words = text.split()\n",
        "    expanded_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Remove punctuation for lookup\n",
        "        #\n",
        "        #\n",
        "        clean_word = re.sub(r'[^\\w/]', '', word)\n",
        "\n",
        "        if clean_word.lower() in MEDICAL_ABBREVIATIONS:\n",
        "            # Replace with expanded term but keep original punctuation\n",
        "            punctuation = re.sub(r'[\\w/]', '', word)\n",
        "            expanded_word = MEDICAL_ABBREVIATIONS[clean_word.lower()] + punctuation\n",
        "            expanded_words.append(expanded_word)\n",
        "        else:\n",
        "            expanded_words.append(word)\n",
        "\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "def standardize_medical_terminology(text, azure_client=None):\n",
        "    \"\"\"Standardize medical terminology using Azure Text Analytics for Health\"\"\"\n",
        "    if azure_client:\n",
        "        try:\n",
        "            # Use Azure Text Analytics for Health\n",
        "            #extracting and standardize medical entities\n",
        "            documents = [text]\n",
        "            response = azure_client.analyze_healthcare_entities(documents)\n",
        "\n",
        "            # processing the recognized entities\n",
        "            result = response[0]\n",
        "            if not result.is_error:\n",
        "                # need to add: incorporate the recognized entities - to standardize terminology\n",
        "                #TO-DO\n",
        "\n",
        "                standardized_text = text\n",
        "                for entity in result.entities:\n",
        "                    if entity.normalized_text and entity.normalized_text != entity.text:\n",
        "                        standardized_text = standardized_text.replace(entity.text, entity.normalized_text)\n",
        "                return standardized_text\n",
        "        except Exception as e:\n",
        "            print(f\"Azure Text Analytics error: {e}\")\n",
        "\n",
        "    # Fallback to basic abbreviation expansion if Azure client is not available\n",
        "    # or if there was an error with the Azure service\n",
        "    return expand_abbreviations(text)\n",
        "    def correct_grammar(text):\n",
        "    \"\"\"Apply grammar correction using TensorFlow model\"\"\"\n",
        "\n",
        "    # NEED TO ADD STILL:TO-DO: you would use a sequence-to-sequence model\n",
        "    # TO-DO:trained specifically for grammar correction\n",
        "\n",
        "    # Simple rule-based corrections as fallback\n",
        "    corrections = [\n",
        "        (r'\\bi\\b', 'I'),  # Capitalize 'i'\n",
        "        (r'\\s+', ' '),    # Remove extra spaces\n",
        "        (r'\\.([a-zA-Z])', '. \\\\1'),  # Add space after period\n",
        "        (r'\\s+\\.', '.'),  # Remove space before period\n",
        "    ]\n",
        "\n",
        "    corrected_text = text\n",
        "    for pattern, replacement in corrections:\n",
        "        corrected_text = re.sub(pattern, replacement, corrected_text)\n",
        "\n",
        "    # Ensure first letter is capitalized\n",
        "    if corrected_text and len(corrected_text) > 0:\n",
        "        corrected_text = corrected_text[0].upper() + corrected_text[1:]\n",
        "\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "7TlExD13qrDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STILL NEED TO APPLY CONTENT MOD AZURE STACK\n",
        "def check_harmful_content(text, azure_client=None):\n",
        "    \"\"\"Check for harmful content using Azure Content Moderator\"\"\"\n",
        "    # TO-DO: use Azure Content Moderator\n",
        "\n",
        "    #CHANGE KEYS\n",
        "    #idk i ust used these\n",
        "    harmful_terms = [\n",
        "        \"overdose\", \"bad\", \"drugs\", \"illegal drugs\", \"abuse\", \"harmful\"\n",
        "    ]\n",
        "\n",
        "    for term in harmful_terms:\n",
        "        if term in text.lower():\n",
        "            return True, f\"Potentially harmful content detected: '{term}'\"\n",
        "\n",
        "    return False, \"\"\n",
        "  def detect_pii(text, azure_client=None):\n",
        "    \"\"\"Detect and handle personally identifiable information using Azure services\"\"\"\n",
        "    # TO DO: would use Azure's PII detection\n",
        "\n",
        "\n",
        "    # Simple pattern matching for common PII\n",
        "    pii_patterns = {\n",
        "        'ssn': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',\n",
        "        'phone': r'\\b\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}\\b',\n",
        "        'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "        'dob': r'\\b(0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])[-/](19|20)\\d{2}\\b'\n",
        "    }\n",
        "\n",
        "    pii_found = {}\n",
        "    for pii_type, pattern in pii_patterns.items():\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            pii_found[pii_type] = matches\n"
      ],
      "metadata": {
        "id": "sj9C8_-5qvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redact PII from text\n",
        "    redacted_text = text\n",
        "    for pii_type, matches in pii_found.items():\n",
        "        for match in matches:\n",
        "            if pii_type == 'ssn':\n",
        "                redacted_text = redacted_text.replace(match, \"[REDACTED SSN]\")\n",
        "            elif pii_type == 'phone':\n",
        "                redacted_text = redacted_text.replace(match, \"[REDACTED PHONE]\")\n",
        "            elif pii_type == 'email':\n",
        "                redacted_text = redacted_text.replace(match, \"[REDACTED EMAIL]\")\n",
        "            elif pii_type == 'dob':\n",
        "                redacted_text = redacted_text.replace(match, \"[REDACTED DOB]\")\n",
        "\n",
        "    return redacted_text, len(pii_found) > 0"
      ],
      "metadata": {
        "id": "axajbTJwq8EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_clinical_text(text, use_azure=False):\n",
        "    \"\"\"Main function to process and standardize clinical text\"\"\"\n",
        "    # Setup Azure client if needed\n",
        "    azure_client = get_azure_text_analytics_client() if use_azure else None\n",
        "\n",
        "    # track the processing steps\n",
        "    processing_steps = []\n",
        "\n",
        "    # Step 1: preprocess text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    if preprocessed_text != text:\n",
        "        processing_steps.append({\n",
        "            \"step\": \"Preprocessing\",\n",
        "            \"before\": text,\n",
        "            \"after\": preprocessed_text\n",
        "        })"
      ],
      "metadata": {
        "id": "CdISSbSjq-4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Check for thee harmful content\n",
        "    is_harmful, harmful_msg = check_harmful_content(preprocessed_text, azure_client)\n",
        "    if is_harmful:\n",
        "        return {\n",
        "            \"original_text\": text,\n",
        "            \"processed_text\": text,\n",
        "            \"standardized\": False,\n",
        "            \"error\": harmful_msg,\n",
        "            \"processing_steps\": processing_steps\n",
        "        }"
      ],
      "metadata": {
        "id": "5CcDBad3rBeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Handle the PII\n",
        "    redacted_text, has_pii = detect_pii(preprocessed_text, azure_client)\n",
        "    if has_pii:\n",
        "        processing_steps.append({\n",
        "            \"step\": \"PII Detection\",\n",
        "            \"before\": preprocessed_text,\n",
        "            \"after\": redacted_text\n",
        "        })\n",
        "        preprocessed_text = redacted_text"
      ],
      "metadata": {
        "id": "L9lJ4xQ9rGME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Standardize all the medical terms\n",
        "    standardized_text = standardize_medical_terminology(preprocessed_text, azure_client)\n",
        "    if standardized_text != preprocessed_text:\n",
        "        processing_steps.append({\n",
        "            \"step\": \"Medical Terminology Standardization\",\n",
        "            \"before\": preprocessed_text,\n",
        "            \"after\": standardized_text\n",
        "        })"
      ],
      "metadata": {
        "id": "kCeaL71nrIL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Correct grammar\n",
        "    final_text = correct_grammar(standardized_text)\n",
        "    if final_text != standardized_text:\n",
        "        processing_steps.append({\n",
        "            \"step\": \"Grammar Correction\",\n",
        "            \"before\": standardized_text,\n",
        "            \"after\": final_text\n",
        "        })\n",
        "return {\n",
        "        \"original_text\": text,\n",
        "        \"processed_text\": final_text,\n",
        "        \"standardized\": True,\n",
        "        \"processing_steps\": processing_steps\n",
        "    }"
      ],
      "metadata": {
        "id": "XXnw32a2rKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask routes\n",
        "#UPDATE TO UR OWN FILE\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/process', methods=['POST'])\n",
        "def process_text():\n",
        "    data = request.get_json()\n",
        "    clinical_text = data.get('text', '')\n",
        "    use_azure = data.get('use_azure', False)\n",
        "\n",
        "    if not clinical_text:\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    result = process_clinical_text(clinical_text, use_azure)\n",
        "    return jsonify(result)\n",
        "\n",
        "@app.route('/examples')\n",
        "def get_examples():\n",
        "    examples = [\n",
        "        \"pt c/o sob after eating\",\n",
        "        \"pt with hx of mi presents with cp\",\n",
        "        \"f/u pt for lab results\",\n",
        "        \"56 yo m with fever and cough\",\n",
        "        \"pt on abx for uti, f/u in 2d\"\n",
        "    ]\n",
        "    processed_examples = []\n",
        "\n",
        "    for example in examples:\n",
        "        result = process_clinical_text(example, False)\n",
        "        processed_examples.append({\n",
        "            \"original\": example,\n",
        "            \"processed\": result[\"processed_text\"]\n",
        "        })\n",
        "\n",
        "    return jsonify(processed_examples)"
      ],
      "metadata": {
        "id": "yVu0UnPzrQJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "1c59LhQJrR3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}